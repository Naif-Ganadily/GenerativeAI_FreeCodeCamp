{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 Huggingface API & Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langchain in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (0.1.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.28 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (0.0.28)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.31 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (0.1.32)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (0.1.27)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (1.24.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.31->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: openai in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (1.14.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from openai) (2.6.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import langchain\n",
    "from langchain.llms import OpenAI\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import os\n",
    "\n",
    "secret_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(openai_api_key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero shot prompting\n",
    "\n",
    "prompt = \"List me 10 largest cities in the United States of America\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. New York City, New York',\n",
       " '2. Los Angeles, California',\n",
       " '3. Chicago, Illinois',\n",
       " '4. Houston, Texas',\n",
       " '5. Phoenix, Arizona',\n",
       " '6. Philadelphia, Pennsylvania',\n",
       " '7. San Antonio, Texas',\n",
       " '8. San Diego, California',\n",
       " '9. Dallas, Texas',\n",
       " '10. San Jose, California']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt).strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template=\"Can you list me the 10 largest cities in {country}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you list me the 10 largest cities in France'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_name.format(country=\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Can you list me the 10 largest cities in China'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template_name.format(country=\"China\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Flask or a Streamlit application that prompts users for a country name and retrieves specific information based on that country. Instead of requiring users to provide a complete prompt, the application utilizes a PromptTemplate to construct the necessary prompt with just a simple variable provided by the user. This functionality cannot be achieved directly through the OpenAI API; it requires integration with Langchain to facilitate the prompt construction and information retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Shanghai',\n",
       " '2. Beijing',\n",
       " '3. Guangzhou',\n",
       " '4. Shenzhen',\n",
       " '5. Chongqing',\n",
       " '6. Tianjin',\n",
       " '7. Chengdu',\n",
       " '8. Nanjing',\n",
       " '9. Wuhan',\n",
       " \"10. Xi'an\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt_template_name.format(country=\"China\")).strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"Whats the current GDP of the United States of America and tell me which year you are referring to?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['As of 2021, the current GDP of the United States of America is approximately $22.68 trillion. This figure is based on the latest data from the World Bank.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.predict(prompt2).strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see I assume its using GPT 3.5 which the cutoff is 2021 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I will extract real time information using Serp API, by using Google search engine I will extract the information in real time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: google-search-results in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from google-search-results) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from requests->google-search-results) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from requests->google-search-results) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from requests->google-search-results) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from requests->google-search-results) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://serpapi.com/\n",
    "# Free Plan Limited to 100 Searches\n",
    "serpapi_key = os.environ.get(\"SERPAPI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(openai_api_key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = load_tools([\"serpapi\"], serpapi_key=serpapi_key, llm=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(tool, client, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should always think about what to do\n",
      "Action: Search\n",
      "Action Input: \"US GDP by year\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[\"The U.S. is a country of 50 states covering a vast swath of North America, with Alaska in the northwest and Hawaii extending the nation’s presence into the Pacific Ocean. Major Atlantic Coast cities are New York, a global finance and culture center, and capital Washington, DC. Midwestern metropolis Chicago is known for influential architecture and on the west coast, Los Angeles' Hollywood is famed for filmmaking. ― Google\", 'United States type: Country in North America.', 'United States entity_type: countries, locations, travel, competitions, military, travel.', 'United States kgmid: /m/09c7w0.', 'United States place_id: ChIJCzYy5IS16lQRQrfeQ5K5Oxw.', 'Gross domestic product (GDP), the featured measure of U.S. output, is the market value of the goods and services produced by labor and property located in the ...', 'U.S. gdp for 2019 was $21,380.98B, a 4.13% increase from 2018. Download Historical Data Save as Image ...', 'Real gross domestic product (GDP) increased at an annual rate of 3.2 percent in the fourth quarter of 2023, according to the “second” estimate.', 'GDP in the United States averaged 8213.43 USD Billion from 1960 until 2022, reaching an all time high of 25439.70 USD Billion in 2022 and a record low of 543.30 ...', 'View the annual rate of economic output, or the inflation-adjusted value of all new goods and services produced by labor and property located in the U.S..', 'U.S. annual GDP 1990-2023 ... In 2023, the U.S. GDP increased from the previous year to about 27.36 trillion U.S. dollars. This increase in GDP ...', 'Most Recent Year. Most Recent Value (Millions). United States. 2022. 25,439,700.00. All Countries and Economies. Country. Most Recent Year. Most Recent Value', 'Gross Domestic Product (GDP) of the United States ; 2019, $21,381,000,000,000, $19,929,000,000,000, 2.29%, $59,611 ; 2018, $20,533,100,000,000 ...', 'GDP growth (annual %) - United States from The World Bank: Data.', 'Real gross domestic product (GDP) increased in all 50 states and the District of Columbia in the third quarter of 2023, with the percent change ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should always think about what to do\n",
      "Action: Search\n",
      "Action Input: \"US GDP 2023\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mCurrent-dollar GDP increased 6.3 percent, or $1.61 trillion, in 2023 to a level of $27.36 trillion, compared with an increase of 9.1 percent, or $2.15 trillion, in 2022 (tables 1 and 3).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should always think about what to do\n",
      "Action: Search\n",
      "Action Input: \"US GDP 2023\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mCurrent-dollar GDP increased 6.3 percent, or $1.61 trillion, in 2023 to a level of $27.36 trillion, compared with an increase of 9.1 percent, or $2.15 trillion, in 2022 (tables 1 and 3).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should always think about what to do\n",
      "Action: Search\n",
      "Action Input: \"US GDP 2023\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mCurrent-dollar GDP increased 6.3 percent, or $1.61 trillion, in 2023 to a level of $27.36 trillion, compared with an increase of 9.1 percent, or $2.15 trillion, in 2022 (tables 1 and 3).\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The current GDP of the United States in 2023 is $27.36 trillion.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The current GDP of the United States in 2023 is $27.36 trillion.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Whats the current GDP of the United States of America and tell me which year you are referring to?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m You should always think about what to do\n",
      "Action: Search\n",
      "Action Input: \"Top current affairs United States of America\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Journalist Mina Al-Oraibi on how the Arab world perceives White House policy on Israel, Hamas, and the Gaza humanitarian crisis.', 'Get the latest news about politics, the economy, health care, the environment, education, Congress and state, local and federal government.', 'Reuters.com is your online source for the latest US news stories and current events, ensuring our readers up to date with any breaking news developments.', 'View the latest national news from The Associated Press. Stay tuned in to all of the current events within the U.S. borders and breaking U.S. news stories.', 'USA TODAY delivers current national and local news, sports, entertainment, finance, technology, and more through award-winning journalism, photos, ...', 'Get the latest news headlines and top stories from NBCNews.com. Find videos and news articles on the latest stories in the US.', 'Breaking news, photos and videos from around the United States. Read our latest coverage on education, health care, immigration, politics, race and ...', 'Read the latest and breaking national US News covering Law, Politics, Education, Crime and College Rankings as well as the impact of climate on the ...', \"View the latest US news, top stories, photos and videos from around the nation. To get the day's top headlines delivered to your inbox every morning, ...\", 'US News is a recognized leader in college, grad school, hospital, mutual fund, and car rankings. Track elected officials, research health conditions, ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The top 5 current affairs in the United States of America are the Arab world's perception of White House policy on Israel, Hamas, and the Gaza humanitarian crisis, updates on politics, the economy, healthcare, environment, and education, breaking news developments from Reuters and The Associated Press, national news from USA TODAY, and current news headlines from NBC News.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The top 5 current affairs in the United States of America are the Arab world's perception of White House policy on Israel, Hamas, and the Gaza humanitarian crisis, updates on politics, the economy, healthcare, environment, and education, breaking news developments from Reuters and The Associated Press, national news from USA TODAY, and current news headlines from NBC News.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Tell me the 5 top current affairs in the United States of America\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What we used was the Google Search Engine now lets try Wikipedia:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: wikipedia in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = load_tools([\"wikipedia\"], llm=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tool, client, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should read about YOLO You Only Look Once on Wikipedia\n",
      "Action: wikipedia\n",
      "Action Input: YOLO You Only Look Once\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Timeline of algorithms\n",
      "Summary: The following timeline of algorithms outlines the development of algorithms (mainly \"mathematical recipes\") since their inception.\n",
      "\n",
      "Page: You Only Live Once (book)\n",
      "Summary: You Only Live Once: The Roadmap to Financial Wellness and a Purposeful Life is a 2016 book written by Jason Vitug. The book sets to redefine the YOLO mantra that has been used to define Generation Y or millennials, to set a mindset shift to cultivate a healthy and wealthy lifestyle for a lifetime. The book focuses on a three step process called ACT, an acronym for awareness, creating a plan, and taking control steps, outlined by the author. The books aim is to make readers define the life they want to live before setting financial goals. You Only Live Once: The Roadmap to Financial Wellness and a Purposeful Life is written in a simple conversation tone based on Vitug's financial experiences and conversations with others\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should read about the YOLO You Only Look Once algorithm on Wikipedia\n",
      "Action: wikipedia\n",
      "Action Input: YOLO You Only Look Once (algorithm)\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Timeline of algorithms\n",
      "Summary: The following timeline of algorithms outlines the development of algorithms (mainly \"mathematical recipes\") since their inception.\n",
      "\n",
      "Page: Small object detection\n",
      "Summary: Small object detection is a particular case of object detection where various techniques are employed to detect small objects in digital images and videos. \"Small objects\" are objects having a small pixel footprint in the input image. In areas such as aerial imagery, state-of-the-art object detection techniques under performed because of small objects.\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should read about the YOLO You Only Look Once algorithm on Wikipedia\n",
      "Action: wikipedia\n",
      "Action Input: YOLO You Only Look Once (algorithm)\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file c:\\Users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Timeline of algorithms\n",
      "Summary: The following timeline of algorithms outlines the development of algorithms (mainly \"mathematical recipes\") since their inception.\n",
      "\n",
      "Page: Small object detection\n",
      "Summary: Small object detection is a particular case of object detection where various techniques are employed to detect small objects in digital images and videos. \"Small objects\" are objects having a small pixel footprint in the input image. In areas such as aerial imagery, state-of-the-art object detection techniques under performed because of small objects.\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: YOLO You Only Look Once (algorithm) is a real-time object detection algorithm that uses a single convolutional neural network to detect objects in images and videos. It has been widely used in the computer vision community and is known for its speed and accuracy.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'YOLO You Only Look Once (algorithm) is a real-time object detection algorithm that uses a single convolutional neural network to detect objects in images and videos. It has been widely used in the computer vision community and is known for its speed and accuracy.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Find me an overview of the YOLO You Only Look Once algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m The best way to answer this question is to use Wikipedia to get accurate information about the current president's education.\n",
      "Action: wikipedia\n",
      "Action Input: Joe Biden\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Joe Biden\n",
      "Summary: Joseph Robinette Biden Jr. ( , BY-dən; born November 20, 1942) is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.\n",
      "Born in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He graduated from the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act. He also oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Obama chose Biden as his running mate, and he was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence. He is the oldest president in U.S. history, and the first to have a female vice president.\n",
      "As president, Biden signed the American Rescue Plan Act in response to the COVID-19 pandemic and subsequent recession. He signed bipartisan bills on infrastructure and manufacturing. He proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that he signed into law in 2022. Biden appointed Ketanji Brown Jackson to the Supreme Court. He worked with congressional Republicans to resolve the 2023 United States debt-ceiling crisis by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America's membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. During the Israel–Hamas war, Biden announced military support for Israel, condemned the actions of Hamas and other Palestinian militants as terrorism, and ordered US military to build a port to facilitate the arrival of humanitarian aid to Palestinian civilians in Gaza. In April 2023, Biden announced his candidacy for the Democratic nomination in the 2024 presidential election.\n",
      "\n",
      "Page: Family of Joe Biden\n",
      "Summary: Joe Biden, the 46th and current president of the United States, has family members who are prominent in law, education, activism and politics. Biden's immediate family became the first family of the United States on his inauguration on January 20, 2021. His immediate family circle was also the second family of the United States from 2009 to 2017, when Biden was vice president. Biden's family is mostly descended from the British Isles, with most of their ancestors coming from Ireland and England, and a smaller number descending from the French.Of Joe Biden's sixteen great-great-grandparents, ten were born in Ireland. He is descended from the Blewitts of County Mayo and the Finnegans of County Louth. One of Biden's great-great-great-grandfathers was born in Sussex, England, and emigrated to Maryland in the United States by 1820.\n",
      "\n",
      "\n",
      "\n",
      "Page: Presidency of Joe Biden\n",
      "Summary: Joe Biden's tenure as the 46th president of the United States began with his inauguration on January 20, 2021. Biden, a Democrat from Delaware who previously served as vice president for two terms under President Barack Obama, took office following his victory in the 2020 presidential election over Republican incumbent president Donald Trump. Upon his inauguration, he became the oldest president in A\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m From the information gathered, it seems that Joe Biden attended the University of Delaware and Syracuse University for law school.\n",
      "Action: wikipedia\n",
      "Action Input: Joe Biden education\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Joe Biden\n",
      "Summary: Joseph Robinette Biden Jr. ( , BY-dən; born November 20, 1942) is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.\n",
      "Born in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He graduated from the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act. He also oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Obama chose Biden as his running mate, and he was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence. He is the oldest president in U.S. history, and the first to have a female vice president.\n",
      "As president, Biden signed the American Rescue Plan Act in response to the COVID-19 pandemic and subsequent recession. He signed bipartisan bills on infrastructure and manufacturing. He proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that he signed into law in 2022. Biden appointed Ketanji Brown Jackson to the Supreme Court. He worked with congressional Republicans to resolve the 2023 United States debt-ceiling crisis by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America's membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. During the Israel–Hamas war, Biden announced military support for Israel, condemned the actions of Hamas and other Palestinian militants as terrorism, and ordered US military to build a port to facilitate the arrival of humanitarian aid to Palestinian civilians in Gaza. In April 2023, Biden announced his candidacy for the Democratic nomination in the 2024 presidential election.\n",
      "\n",
      "Page: Cabinet of Joe Biden\n",
      "Summary: Joe Biden assumed office as President of the United States on January 20, 2021. The president has the authority to nominate members of his Cabinet to the United States Senate for confirmation under the Appointments Clause of the United States Constitution.\n",
      "Before confirmation and during congressional hearings, a high-level career member of an executive department heads this pre-confirmed cabinet on an acting basis. The Cabinet's creation was part of the transition of power following the 2020 United States presidential election.\n",
      "In addition to the 15 heads of executive departments, there are 10 Cabinet-level officials. Biden altered his cabinet structure, elevating the chair of the Council of Economic Advisers, director of the Office of Science and Technology Policy and ambassador to the United Nations as Cabinet-level positions. Biden initially removed the director of the Central Intelligence Agency from his Cabinet, but reversed the move in July 2023.Confirmations had occurred at the slowest pace of any presidential cabinet in modern history that resulted from delays in facilitating an orderly transition of power and passing the organizing resolution for governing an evenly split Senate following the 2020–2021 United States Senate runoff elections in Georgia; an\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m From the information gathered, it seems that Joe Biden attended the University of Delaware and Syracuse University for law school.\n",
      "Action: wikipedia\n",
      "Action Input: Joe Biden education\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPage: Joe Biden\n",
      "Summary: Joseph Robinette Biden Jr. ( , BY-dən; born November 20, 1942) is an American politician who is the 46th and current president of the United States. A member of the Democratic Party, he previously served as the 47th vice president from 2009 to 2017 under President Barack Obama and represented Delaware in the United States Senate from 1973 to 2009.\n",
      "Born in Scranton, Pennsylvania, Biden moved with his family to Delaware in 1953. He graduated from the University of Delaware before earning his law degree from Syracuse University. He was elected to the New Castle County Council in 1970 and to the U.S. Senate in 1972. As a senator, Biden drafted and led the effort to pass the Violent Crime Control and Law Enforcement Act and the Violence Against Women Act. He also oversaw six U.S. Supreme Court confirmation hearings, including the contentious hearings for Robert Bork and Clarence Thomas. Biden ran unsuccessfully for the Democratic presidential nomination in 1988 and 2008. In 2008, Obama chose Biden as his running mate, and he was a close counselor to Obama during his two terms as vice president. In the 2020 presidential election, Biden and his running mate, Kamala Harris, defeated incumbents Donald Trump and Mike Pence. He is the oldest president in U.S. history, and the first to have a female vice president.\n",
      "As president, Biden signed the American Rescue Plan Act in response to the COVID-19 pandemic and subsequent recession. He signed bipartisan bills on infrastructure and manufacturing. He proposed the Build Back Better Act, which failed in Congress, but aspects of which were incorporated into the Inflation Reduction Act that he signed into law in 2022. Biden appointed Ketanji Brown Jackson to the Supreme Court. He worked with congressional Republicans to resolve the 2023 United States debt-ceiling crisis by negotiating a deal to raise the debt ceiling. In foreign policy, Biden restored America's membership in the Paris Agreement. He oversaw the complete withdrawal of U.S. troops from Afghanistan that ended the war in Afghanistan, during which the Afghan government collapsed and the Taliban seized control. He responded to the Russian invasion of Ukraine by imposing sanctions on Russia and authorizing civilian and military aid to Ukraine. During the Israel–Hamas war, Biden announced military support for Israel, condemned the actions of Hamas and other Palestinian militants as terrorism, and ordered US military to build a port to facilitate the arrival of humanitarian aid to Palestinian civilians in Gaza. In April 2023, Biden announced his candidacy for the Democratic nomination in the 2024 presidential election.\n",
      "\n",
      "Page: Cabinet of Joe Biden\n",
      "Summary: Joe Biden assumed office as President of the United States on January 20, 2021. The president has the authority to nominate members of his Cabinet to the United States Senate for confirmation under the Appointments Clause of the United States Constitution.\n",
      "Before confirmation and during congressional hearings, a high-level career member of an executive department heads this pre-confirmed cabinet on an acting basis. The Cabinet's creation was part of the transition of power following the 2020 United States presidential election.\n",
      "In addition to the 15 heads of executive departments, there are 10 Cabinet-level officials. Biden altered his cabinet structure, elevating the chair of the Council of Economic Advisers, director of the Office of Science and Technology Policy and ambassador to the United Nations as Cabinet-level positions. Biden initially removed the director of the Central Intelligence Agency from his Cabinet, but reversed the move in July 2023.Confirmations had occurred at the slowest pace of any presidential cabinet in modern history that resulted from delays in facilitating an orderly transition of power and passing the organizing resolution for governing an evenly split Senate following the 2020–2021 United States Senate runoff elections in Georgia; an\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Joe Biden attended the University of Delaware and Syracuse University for law school.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Joe Biden attended the University of Delaware and Syracuse University for law school.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Give me a education overview of the current president of the United States of America\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** <br>\n",
    "Central to LangChain is a vital component known as LangChain Chains, forming the core connection among one or several Large Language Models (LLMs). In certain sophisticated applications, it becomes necessart to chain LLMs together, either to enhance the quality of the generated text or to enable the generation of text that is beyond the capability of a single LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAI(client=<openai.resources.completions.Completions object at 0x00000297D2AF0D60>, async_client=<openai.resources.completions.AsyncCompletions object at 0x00000297D2B04FD0>, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example using Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=client, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"IntelliWrite Solutions\" '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"AI SaaS for White Papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example using Chains 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"service\"],\n",
    "    template=\"I want to open an AI as a Service company that offers this {service}. What should I name it?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['service'], template='I want to open an AI as a Service company that offers this {service}. What should I name it?')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain2 = LLMChain(llm=client, prompt=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. \"CortexAI Solutions\"',\n",
       " '2. \"MindHive AI Services\"',\n",
       " '3. \"NeuralNet Innovations\"',\n",
       " '4. \"IntelliServe AI\"',\n",
       " '5. \"Cognitive Cloud Co.\"',\n",
       " '6. \"AI Nexus Services\"',\n",
       " '7. \"SmartBrain Solutions\"',\n",
       " '8. \"DeepMind Industries\"',\n",
       " '9. \"InnovAIte Services\"',\n",
       " '10. \"VirtualBrain Co.\"']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2.run(\"White Papers\").strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain3 = LLMChain(llm=client, prompt=prompt_template, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI want to open an AI as a Service company that offers this Legal Documents. What should I name it?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. \"AI LegalDocs\"\\n2. \"DocuAI\"\\n3. \"LegalLeverage AI\"\\n4. \"SmartDocs Solutions\"\\n5. \"AI Law Co.\"\\n6. \"LegalMind AI\"\\n7. \"DocuTech AI\"\\n8. \"AI LegalEase\"\\n9. \"LegalGenius AI\"\\n10. \"SmartLegal Docs\"'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain3.run(\"Legal Documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you want to combine multiple chain and set a sequence for that we use a simple sequential chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name2 = PromptTemplate(\n",
    "    input_variables=[\"service\"],\n",
    "    template=\"I want to open an AI as a platform as a service company that offers this {service}. What should I name it?\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=client, prompt=prompt_template_name2)\n",
    "\n",
    "prompt_template_items= PromptTemplate(\n",
    "    input_variables=[\"name\"],\n",
    "    template=\"Suggest some strategies for {name}\"\n",
    ")\n",
    "\n",
    "strategies_chain = LLMChain(llm=client, prompt=prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining both chains\n",
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain=SimpleSequentialChain(chains = [name_chain, strategies_chain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "As for strategies, here are a few suggestions:\n",
      "\n",
      "1. Define your target audience: Identify the specific segment of the legal industry that you want to cater to, such as law firms, corporate legal departments, or individual lawyers. This will help you tailor your marketing and sales efforts accordingly.\n",
      "\n",
      "2. Highlight the benefits: Showcase how your platform can make legal processes more efficient, accurate, and cost-effective for your potential clients. This could include features such as document review, contract analysis, or legal research.\n",
      "\n",
      "3. Partner with established legal organizations: Collaborate with bar associations, legal technology companies, or law schools to gain credibility and reach a wider audience.\n",
      "\n",
      "4. Offer free trials and demos: Allow potential clients to test out your platform for a certain period or offer live demonstrations to showcase its capabilities. This can help build trust and increase conversions.\n",
      "\n",
      "5. Utilize social media and content marketing: Use social media platforms like LinkedIn and Twitter to connect with legal professionals and share educational content about the benefits of your platform. This can help establish your company as a thought leader in the legal AI space.\n",
      "\n",
      "6. Attend legal industry events: Participate in conferences, seminars, and trade shows related to the legal industry to network and showcase your platform to potential clients.\n",
      "\n",
      "7. Provide excellent customer\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"Legal AI\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draw Back:** <br>\n",
    "Gave us an answer from the second prompt and not both prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables=[\"service\"],\n",
    "    template=\"I want to open an AI as a platform as a service company that offers this {service}. What should I name it?\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=client, prompt=prompt_template_name, output_key=\"name of the company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_items = PromptTemplate(\n",
    "    input_variables=[\"name of the company\"],\n",
    "    template=\"Suggest some services for {name of the company}\"\n",
    ")\n",
    "\n",
    "items_chain = LLMChain(llm=client, prompt=prompt_template_items, output_key=\"elevator pitch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = SimpleSequentialChain(chains=[name_chain, items_chain])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. AI development and consulting services',\n",
       " '2. AI incubator program for startups and entrepreneurs',\n",
       " '3. AI strategy and implementation consulting',\n",
       " '4. AI software development and customization',\n",
       " '5. AI training and education programs',\n",
       " '6. AI data management and analytics services',\n",
       " '7. AI integration and deployment services',\n",
       " '8. AI networking and collaboration platform',\n",
       " '9. AI marketing and branding services',\n",
       " '10. AI ethical and responsible AI consulting and training.']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Input\").strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Document Loader "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://python.langchain.com/docs/modules/data_connection/document_loaders/\n",
    "\n",
    "Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\n",
    "\n",
    "Document loaders provide a \"load\" method for loading data as documents from a configured source. They optionally implement a \"lazy load\" as well for lazily loading data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-4.1.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.7.4.3 in c:\\users\\ganad\\anaconda3\\envs\\testingopenai\\lib\\site-packages (from pypdf) (4.10.0)\n",
      "Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n",
      "   ---------------------------------------- 0.0/286.1 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 41.0/286.1 kB 653.6 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 143.4/286.1 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 143.4/286.1 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 143.4/286.1 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 143.4/286.1 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 143.4/286.1 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------- ----------------- 153.6/286.1 kB 459.5 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 153.6/286.1 kB 459.5 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 153.6/286.1 kB 459.5 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 225.3/286.1 kB 474.7 kB/s eta 0:00:01\n",
      "   ---------------------------------- --- 256.0/286.1 kB 491.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 286.1/286.1 kB 535.6 kB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(r\"C:\\Users\\ganad\\Desktop\\Learning with Projects\\GenerativeAI_FreeCodeCamp-1\\𝐃𝐀𝐘_𝟒_𝐇𝐮𝐠𝐠𝐢𝐧𝐠𝐅𝐚𝐜𝐞_𝐀𝐏𝐈_&_𝐋𝐚𝐧𝐠𝐜𝐡𝐚𝐢𝐧\\EEP567_FFMMMTI_Final_Report (1).pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyPDFLoader at 0x1f0245083a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='1 Friend or Foe: Multi -Modal Military Target Identification  \\nAndrew Jeon • Bassam Halabiya • Naif Ganadily • Zachary Saunders  \\nUniversity of Washington  \\nEE P 567: Machine Learning for Cybersecurity  \\nProfessor Radha Poovendran  • Lead  TA Qifan Lu  \\nRoles and Responsibilities  \\nWe constructed our custom dataset together in equal parts, taking images mainly from Google , following \\nthe split below to create our initial 1100 image dataset.  \\n● Andrew - US/RU Navy (250 Images) + Landscape/Urban “Null” (25 images)  \\n● Bassam - US/RU Marines (250 Images) + Paintball “Null” (25 images)  \\n● Naif - US/RU Army (250 Images) + Desert “Null” (25 images)  \\n● Zachary - US/RU Airforce (250 Images) + Civilians “Null” (25 images)  \\n \\nBassam took a leadership role in dataset construction , leveraging his expertise with Roboflow and dataset \\ncreation. He oversaw the quality control process for our dataset, meticulously providing feedback for \\nrefinement upon submission. He also managed most of the data augmentations on Roboflow to increase \\nour dataset size to 2,652 images.  With  the dataset ready , we divided responsibilities for developing the \\nMachine Learning model. A ndrew, Bassam, and Naif collaborated on image classification into \\nRU_milita ry or US_military  classes while Zachary focused on developing  the Challenge  Response \\nSystem. Andrew, Bassam, and Naif experimented with YOLOv 8 architectures to achieve and deliver \\ninitial detection/classification results. More specifically, A ndrew employed  YOLOv8Nano, Bassam \\nutilized  YOLOv8Small  and YOLOv8Medium , and Naif  experimented with YOLOv8Large and \\nYOLOv8Extra Large. After initial modeling, Bassam and Naif led the refinement and optimization of the \\nmodels to improve overall performance.  \\n \\nRegarding the written documentation, the project’s inception was credited to Zachary, who took the lead \\nposition in drafting the proposal while incorporating feedback from the team. As for the checkpoint and \\nfinal project reports, the team divided responsibilities equally among team members. For the checkpoint \\nreport, Andrew wrote about additional experiments, Bassam summarized  preliminary conclusions, Naif \\noutlined the  milestones, and Zachary covered the main bottleneck s encountered . For the final report, \\nAndrew focused on roles & responsibilities, references, and code documentation. Zachary drafted the \\nproblem statement and our proposal for tackling it. Naif contributed to the YOLOv8 architecture and \\nSegment Anything Model (SAM) . Finally, Bassam provided insights into e xperimental results & analysis \\nas well as future work.  To provide proof of concept, Bassam and Naif developed two applications to \\nshowcase real-world  scenarios. Naif’s devel opment of a user -friendly web application using Streamlit \\nshowcased the accuracy  and robustness of the YOLOv8x for recorded video and image analysis. On the \\nother hand, Bassam integrated the challenge -response system with the AI model to showcase the \\nsuitability and reliability of the YOLOv8s model for real -time applications.', metadata={'source': 'C:\\\\Users\\\\ganad\\\\Desktop\\\\Learning with Projects\\\\GenerativeAI_FreeCodeCamp-1\\\\𝐃𝐀𝐘_𝟒_𝐇𝐮𝐠𝐠𝐢𝐧𝐠𝐅𝐚𝐜𝐞_𝐀𝐏𝐈_&_𝐋𝐚𝐧𝐠𝐜𝐡𝐚𝐢𝐧\\\\EEP567_FFMMMTI_Final_Report (1).pdf', 'page': 0}),\n",
       " Document(page_content=\"2  \\nContext & Problem Statements  \\nWar is unpredictable; despite months of planning, steadfast discipline, and constant  communication, any \\nfighting force is only the sum of its parts, fallible and imperfect humans.  Physical exhaustion, emotional \\ndistress, and unfamiliar environments notwithstanding, during  combat soldiers are faced with a choice: to \\nshoot a target or not to shoot. Superficially this  proposition appears trivial: if the target is an enemy, fire, \\notherwise do not. However, it is not so.  On the modern battlefield, combat occurs from hundreds of yards \\naway, with rifles aimed at  soldiers wearing camouflage, belonging to different factions, and organized in \\nguerrilla  formations, intentionally designed to challenge target identification. With varying degrees of  \\ntrickery at play, mistakes do happen. Friendly fire and unnecessary casualties due to mistaken  target \\nidentification have become a facet of warfare. “Many Americans [are] shocked to learn  that 23 percent of \\nall [American] casualties in the Gulf War were from [American] Weapons. ” (Shrader) Military \\nengagements can be won or lost by a factor far less than 23%, and therefore  should there be a method by \\nwhich soldiers may positively identify friend from foe, such  technology would have a monumental impact \\non the success of their mission objectives.  \\n \\nWith a clear need for a more effective target identification system, we explored existing solutions, \\nanalyzed standard issue equipment, and reviewed military doctrine, all in an effort to not only develop a \\nsolution that performed well but also seamlessly integrated with existing battlefield technologies. \\nThrough this research, we discovered that the most feasible solution for target identification involved \\nvisually capturing images through a camera, analyzing the captured images, and reporting to the user,  the \\nsoldier in this case, the end result: is the target friend or foe? With this plan developed, more questions \\narise. Where do we place the camera in relation to the soldier? How does the camera detect friend and \\nfoe, and what factors will be used in thi s decision? How can such a system be protected and secured from \\nadverse use? Beginning with the camera placement, many solutions were considered, however it was \\nultimately decided that two possible placements provided the optimal line -of-sight necessary to  capture \\nthe target properly: the camera should either be attached to the soldier's helmet akin to a Microsoft \\nHoloLens apparatus or attached to the optic of the firearm similar to a thermal imaging sensor. Although \\nboth solutions have their merits, having  the camera affixed to the firearm allows the soldier to remain \\nbehind cover, being protected from enemy fire, while having the firearm and camera sensor engaged on \\nthe targets. With the camera in place, we then reviewed possible methods for target identif ication. The \\ncore problem with target identification is that of diversity; targets can be young, old, hairy, clean -shaven, \\ntall, short, standing, crouching, etc. With so many variables, it is not feasible to develop a manual \\nsolution, instead, we must empl oy the use of AI. “AI is inherently a decision -making technology: it offers \\nopportunities to automate many tasks relating to learning and devising solutions.”(Verganti et al., 2020) \\nWith an AI solution, our system would be capable of identifying its own si gnificant features, which \\noptimize the friend -or-foe determination, features that we have not manually defined. These features  are \\ndeveloped in a “black -box” manner whereas we do not have visibility into exactly what features are in \\nplay, however, we can influence the AI model via the training data provided. As such, selecting proper \\ntraining data is paramount. We chose to develop  our dataset from images sourced online, augmented, and \\nlabeled according to the military faction that they contain.\", metadata={'source': 'C:\\\\Users\\\\ganad\\\\Desktop\\\\Learning with Projects\\\\GenerativeAI_FreeCodeCamp-1\\\\𝐃𝐀𝐘_𝟒_𝐇𝐮𝐠𝐠𝐢𝐧𝐠𝐅𝐚𝐜𝐞_𝐀𝐏𝐈_&_𝐋𝐚𝐧𝐠𝐜𝐡𝐚𝐢𝐧\\\\EEP567_FFMMMTI_Final_Report (1).pdf', 'page': 1}),\n",
       " Document(page_content='3 With the AI model classifying friend and foe, we must consider adversarial efforts to defeat or co -opt the \\nsystem. Should our system have any success, we can assume that the enemy forces will invest time and \\neffort to neutralize its effectiveness to regain  the upper hand. It then follows that we enhance the AI \\nsystem with a secondary security factor. We again revisited our research to develop an enhancement \\nsolution that would be natural to our soldiers but also provide an added layer of defense. Echoing th e \\nchallenge -response system already used in forward operating bases to verify the legitimacy of a detached \\nsoldier reentering a friendly installation, we developed a challenge -response system (CRS) that \\nautomatically confirms the suspected friendly as friendly and  draws suspicion to a foreign soldier \\nattempting to confuse the AI system. Whereas the AI system is attached to the firearm optic, the CRS has \\ntwo parts: a directional antenna and a software -defined radio (SDR). Additionally, all soldiers’ uniforms  \\nwill now be equipped with embedded radio receivers and transponders. Much like an optic system, the \\nantenna and SDR will point at a target in -plane with the firearm. When aimed at a target, the SDR will \\nissue a challenge radio signal towards the direction  of the target. If aimed at a friendly, the transponder \\nwill hear the signal and respond , confirming that they are indeed friendly. Although this second factor \\nprovides the confirmation we desire, it also introduces a larger attack service for the enemy to  combat. \\nThe most obvious challenge with all this radio traffic (challenge, response, challenge, response, over and \\nover across the radio waves) is the detectable signals being emitted. Should the enemy have radio towers \\nor other equipment on the outskirts  of their facilities, they would trivially be able to predict an attack and \\nperhaps even gather the number of incoming soldiers, information that will jeopardize the lives of our \\nfriendly soldiers. Therefore,  we have also implemented a Stealth Mode in the CRS system that will \\ntemporarily quiet all radio traffic during times of sensitive missions. Furthermore, we are aware of the \\nrisk that spoofing provides. If a friendly soldier is downed in combat, we need a means to eliminate that \\nnode from the CRS pool, this is handled via a timeout function. Furthermore, each CRS transponder uses \\na unique ID which makes spoofing even less possible.  \\n \\nWith this multi -pronged approach to target identification, 3 overall problems needed to be addressed:  \\n \\n1. Collect, label, and ingest an image dataset of soldiers dressed in combat uniforms.  \\n2. Design and develop an artificial intelligence (AI) algorithm that will correctly classify the images \\nas either friendly or foe.  \\n3. Design and develop an enhancement challenge -response system  (CRS) that protects against \\nadversarial efforts and can adapt to different mission objectives.  \\n \\n \\nFigure 1: Firearm with AI Optic & CRS Components Installed  \\nImage source provided in CRS Jupyter Notebook', metadata={'source': 'C:\\\\Users\\\\ganad\\\\Desktop\\\\Learning with Projects\\\\GenerativeAI_FreeCodeCamp-1\\\\𝐃𝐀𝐘_𝟒_𝐇𝐮𝐠𝐠𝐢𝐧𝐠𝐅𝐚𝐜𝐞_𝐀𝐏𝐈_&_𝐋𝐚𝐧𝐠𝐜𝐡𝐚𝐢𝐧\\\\EEP567_FFMMMTI_Final_Report (1).pdf', 'page': 2}),\n",
       " Document(page_content=\"4 SAM  \\nThe SAM (Segment Anything Model) presented by Meta offers a cutting -edge approach to image \\nsegmentation and annotation, At its core, SAM employs a unique network architecture that efficiently \\nhandles complex visual information. This involves a picture enco der transforming 1024x1024 images into \\ncomprehensive multi -channel feature maps, coupled with a prompt encoder that converts various  prompts \\ninto vector embeddings. These components work together within a mask decoder layer to generate three \\ndistinct segme ntation masks at varying levels of granularity, enhancing the model’s precision even in \\nambiguous scenarios. Key to SAM’s efficacy is its innovative use of loss functions, specifically the \\nIntersection over Union (IOU) score, dice loss, and focal loss, whi ch together optimize mask accuracy \\nand address model performance on challenging examples. The architecture’s efficiency stems from its \\nability to pre -compute and reuse image embeddings for different prompts, thanks to the independent \\nprocessing within the mask decoder. Further distinguishing SAM is its incorporation of a vision \\ntransformer -based image encoder, facilitating the translation of images into embeddings with remarkable \\naccuracy. This approach, inspired by foundational work in the field, enables t he model to reconstruct \\nimages from a fraction of the original data, underscoring its robustness. The dataset fueling SAM spans \\nmanual, semi -automatic, and fully automatic stages, amassing a vast collection of annotated images that \\nrefine the model’s capab ilities through progressive training phases. SAM’s architecture intricately blends \\nimage and mask encoders, along with innovative prompt encoding techniques for sparse and bounding \\nbox prompts, and a pre-trained  CLIP model for text prompts. This comprehensive encoding framework \\nensures, a contextual understanding of visual information. The mask decoder’s design employs multiple \\nattention mechanisms to integrate prompt and image embeddings, assembling in high -quality \\nsegmentation masks. This system’s effecti veness is evident across a range of tasks. In essence, SAM \\nexemplifies a profound leap in image segmentation technology that we used for our annotations of the \\nimages through Roboflow’s UI.  \\nYOLOv8 for Friend or Foe  \\nWe utilized the YOLO (You Only Look Once) model, particularly version 8, YOLOv8 tackle the intricate \\ntask of differentiating between ally and enemy forces in military combat. This state-of-the-art algorithm is \\nrenowned for its effectiveness and precision in real-time object detection. It functions by dividing input \\nimages into a grid structure, where each grid cell is responsible for predicting bounding boxes and class \\nprobabilities. These predictions encompass bounding boxes situated at the center of each grid cell \\nalongside confidence scores indicating object presence and class probability maps hinting at the object \\ncategory within each cell.  \\nYOLOv8 Architecture  \\nThe YOLOv8 model constructed on a Convolutional Neural Network (CNN) architecture converts input  \\nimages into a sequence of output vectors. These vectors encapsulate class probabilities and bounding box \\nparameters for carrying out object detection tasks. By abstracting the image through layers into a \\nsophisticated representation this model generates o utput vectors for every grid cell based on high-level  \\nfeatures derived from the input images. This transition from visual data to detailed classifications \\nshowc ases the model's  ability to rapidly and accurately interpret complex  visual information.\", metadata={'source': 'C:\\\\Users\\\\ganad\\\\Desktop\\\\Learning with Projects\\\\GenerativeAI_FreeCodeCamp-1\\\\𝐃𝐀𝐘_𝟒_𝐇𝐮𝐠𝐠𝐢𝐧𝐠𝐅𝐚𝐜𝐞_𝐀𝐏𝐈_&_𝐋𝐚𝐧𝐠𝐜𝐡𝐚𝐢𝐧\\\\EEP567_FFMMMTI_Final_Report (1).pdf', 'page': 3}),\n",
       " Document(page_content=\"5 Fine-tuning  the YOLOv8  \\nA crucial element of our project was in finetuning the YOLOv8 to cater to our requirements with a focus \\non distinguishing among different military personnel. We trained the model using our custom  dataset to \\nimprove  the accuracy of identifying friendly units and adversaries through bounding boxes and fine-\\ntuning  confidence scores to ensure our model could effectively differentiate between friends and foes in \\nvarious operational scenarios.  \\nInference Process  \\nDuring the inference stage , YOLOv8 utilizes maximum suppression to refine its predictions by reducing \\nredundant bounding box detections and prioritizing highly confident predictions. This method  is vital in \\nsettings where precision and quick decision -making  can be critical for saving lives. We fine-tuned  the \\nmodel  parameters incorporating localization and confidence losses to effectively penalize predictions and \\nenhance overall performance and reliability.  \\nExperimental Results and Analysis  \\nIn our computer vision -based machine learning project aimed at assisting soldiers on the battlefield, we \\nconducted extensive experimentation with our custom dataset comprised of 1000 images representing the \\n4 branches of each military faction. Using our cu stom dataset allowed us to tailor it to the project's needs \\nand requirements thereby enabling flexibility concerning annotation, segmentation, and class balance. \\nThis dataset was created, annotated, and deployed on Roboflow where it can be modified to acco mmodate \\nvarious computer vision techniques, and manipulated to have user -specified training, validation, and \\ntesting split. This dataset is made more robust by adding augmentation and pre -processing techniques. For \\nour project, we chose to implement random  rotation, random Gaussian blur, salt & pepper noise, and \\nhorizontal flip to ensure the model can generalize better to unseen data. Our work focused on performing \\ntransfer learning with several pre -trained YOLOv8 models.  \\n  \\nBefore discussing the training and deployment of our model, it’s paramount to address the safety -critical \\naspect nature of the application we are developing. Given the context of military operations where \\ninstantaneous decisions can carry significant ramif ications, the reliability, accuracy, and robustness of our \\nsystem are essential to its success and the livelihood of those involved. False positives or \\nmisclassification of targets can result in compromising the mission or worse, friendly fire.  \\nDataset Details  \\nThe team compiled 1100 images of military soldiers sourced from the internet and each team member was \\nresponsible for collecting and annotating 275 images. The individual team member dataset comprised 125 \\nimages per specific branch for both military forces plus 25 null images. Each image was annotated using \\nRoboflow’s annotation assistant, the Smart Polygon tool, to allow users the flex ibility of training for \\nsegmentation, detection, or both. The team anticipated that adding augmentation techniques would \\nenlarge the dataset to 4000+ images, however, this kind of extensive augmentation was reserved for the \\npaid Roboflow memberships. The c ustom dataset contained 2652 images where 2328 images were \\nassigned for training, 220 images for validation, and 104 images for testing. In Roboflow, several dataset\", metadata={'source': 'C:\\\\Users\\\\ganad\\\\Desktop\\\\Learning with Projects\\\\GenerativeAI_FreeCodeCamp-1\\\\𝐃𝐀𝐘_𝟒_𝐇𝐮𝐠𝐠𝐢𝐧𝐠𝐅𝐚𝐜𝐞_𝐀𝐏𝐈_&_𝐋𝐚𝐧𝐠𝐜𝐡𝐚𝐢𝐧\\\\EEP567_FFMMMTI_Final_Report (1).pdf', 'page': 4}),\n",
       " Document(page_content=\"6 versions were created using the same source images. Table 1 outlines the three major dataset projects that \\nthe team generated and trained in Roboflow.  \\n \\nTable 1: Custom Datasets Published on Roboflow  \\nDataset Name  Classes  Versions  Deployed Models  \\nFriend_or_Foe_Merged  RU_army, RU_airforce, RU_marines, RU_navy, \\nUS_army, US_airforce, US_marines, US_navy  1 1 \\nFriend_or_Foe_Class_\\nConsolidation_ObjDet  RU_military and US_military  10 6 \\nFriend_or_Foe_Class_\\nConsolidation_InstSeg  RU_military and US_military  2 2 \\nDataset issues  \\nOur initial results for the machine learning model suffered from a few setbacks primarily due to issues \\nwith the dataset quality. The combination of these issues resulted in the poor performance of our \\ncomputer vision model as it struggled to accurately di stinguish friendly forces from foes. That said, due to \\ntime limitations and work priority, we were only able to address a few of the issues:  \\n● Difficulty in acquiring images for the Russian military compared to the US military.  \\n● Russian military images suffered from duplications, obscurity, and misalignments compared to \\nthe US military counterparts.   \\n● Blurry images were exacerbated by the data augmentation techniques applied.  \\n● Overly ambitious use of 8 classes to categorize annotations by military factions and branches  \\n● Completeness optimization led to the inclusion of partial body parts for camouflage identification  \\nTraining Details  \\nSeveral, but not all, detection and segmentation YOLOv8 models were utilized to accomplish this \\ncomplex task of distinguishing between camouflage uniforms. The team took a meticulous approach to \\nfine-tuning the YOLOv8 hyperparameters by taking into account  epochs, learning rate, learning rate \\nscheduler, loss function, optimizer, image size, and more. This effort was aimed at finding the balance \\nbetween computational efficiency, accuracy, and resilience while assessing the applicability for real -time \\nusage. The rigorous and iterative optimization process resulted in determining the ideal set of \\nhyperparameters necessary for our system requirements.  \\n  \\nTable 2: Ideal Hyperparameters for Training with YOLOv8  \\nParameter  Value  Ultralytics Description  [2] \\nEpochs  25-50 Total number of training epochs. Each epoch represents a full pass over the dataset. \\nAdjusting this value affects training duration & model performance.  \\nBatch  16 Batch size for training, indicating how many images are processed before the model's \\ninternal parameters are updated.  \\nImgsz  640 Target image size for training. All images are resized to this dimension before being fed \\ninto the model. Affects model accuracy and computational complexity.  \\nOptimizer  SGD  Options include SGD, Adam, AdamW, NAdam, RAdam, RMSProp etc., or auto for \\nautomatic selection based on model configuration.  \\nLr0 0.001  Initial learning rate (i.e. SGD=1E -2, Adam=1E -3). Adjusting this is crucial for the \\noptimization process, influencing how rapidly model weights are updated.\", metadata={'source': 'C:\\\\Users\\\\ganad\\\\Desktop\\\\Learning with Projects\\\\GenerativeAI_FreeCodeCamp-1\\\\𝐃𝐀𝐘_𝟒_𝐇𝐮𝐠𝐠𝐢𝐧𝐠𝐅𝐚𝐜𝐞_𝐀𝐏𝐈_&_𝐋𝐚𝐧𝐠𝐜𝐡𝐚𝐢𝐧\\\\EEP567_FFMMMTI_Final_Report (1).pdf', 'page': 5}),\n",
       " Document(page_content='7 Lrf 0.01 Final learning rate as a fraction of the initial rate = (lr0 * lrf), used in conjunction with \\nschedulers to adjust the learning rate over time.  \\ndropout  0.0 - 0.1 Dropout rate for regularization in classification tasks, preventing overfitting by randomly \\nomitting units during training.  \\nEvaluation Details  \\nTo evaluate our model’s performance, we used recall, precision, and mAP. As a reminder, mAP refers to \\nthe mean Average Precision metric across all classes, while precision refers to how often the model \\npredictions are correct (accurate positive predictions ), and recall refers to the percentage of successfully \\nidentified labeled images (positive prediction completeness). The initial results demonstrate the learning \\nprocess achieved in this project where we were able to eventually obtain a model capable of ac curately \\ndistinguishing friend from foe with a mAP of 75.7%. These promising results pave the way for the \\npossible incorporation into military applications via AI goggles or enhanced weapon optics/scopes.  \\nBy experimenting with multiple YOLOv8 models, we were able to hone down on the hyperparameters \\nthat mattered the most. Table 3 below summarizes the test and experimentation results collected \\nthroughout this project.  \\n \\nTable 3: Transfer Learning Results with YOLOv8  \\nModel  Dataset  Hyperparameters  mAP50  mAP50 -95 \\nYOLOv8n  Friend_or_foe_class_consolidation_objdet/10  Ideal (50 epoch)  0.711  0.536  \\nYOLOv8s  Friend_or_foe_class_consolidation_objdet/7  Ideal (50 epoch)  0.736  0.581  \\nYOLOv8m  Friend_or_foe_class_consolidation_objdet/8  Ideal (50 epoch)  0.773  0.636  \\nYOLOv8l  Friend_or_foe_class_consolidation_objdet/5  Ideal (25 epoch)  0.371  0.307  \\nYOLOv8x  Friend_or_foe_class_consolidation_objdet/9  Ideal (25 epoch)  0.757  0.613  \\nYOLOv8x -seg Friend_or_foe_class_consolidation_objdet/5  Ideal (25 epoch)  0.734  0.562  \\n \\nAfter completing our transfer learning experimentation with YOLOv8 which involved a significant \\namount of trial and error, we were able to streamline our approach by selecting two YOLOv8 models that \\nperformed well within our specifications. The first model, YOLOv8x, is much larger and has \\ndemonstrated greater accuracy when classifying uniforms making it the ideal choice for performing \\ninference on recorded video and im ages. When it comes to image and video analysis, precision is \\npreferred over speed, and this is where this model excels. However, due to its sluggish real -time \\nperformance and computational demands, this model is deemed unsuitable for real -time application s and \\ntherefore impractical for edge device deployment. On the other hand, the YOLOv8s model has proven to \\nbe highly efficient in real -time applications such as performing inference on live video stream frames. \\nAlthough the smaller model is less accurate t han YOLOv8x, its reduced computational overhead makes it \\nideal for deployments to edge devices. The YOLOv8s trained model is well suited to handle field \\noperations usage where speed and accuracy are of equal importance. Using these two models, we can \\ncater  to various applications without having to sacrifice accuracy or speed.', metadata={'source': 'C:\\\\Users\\\\ganad\\\\Desktop\\\\Learning with Projects\\\\GenerativeAI_FreeCodeCamp-1\\\\𝐃𝐀𝐘_𝟒_𝐇𝐮𝐠𝐠𝐢𝐧𝐠𝐅𝐚𝐜𝐞_𝐀𝐏𝐈_&_𝐋𝐚𝐧𝐠𝐜𝐡𝐚𝐢𝐧\\\\EEP567_FFMMMTI_Final_Report (1).pdf', 'page': 6}),\n",
       " Document(page_content='8  \\nFigure 2: YOLOv8s (left) vs YOLOv8x (right) Confusion Matrices  \\n \\n \\nFigure 3: YOLOv8s (Top) vs YOLOv8x (bottom) Training Results', metadata={'source': 'C:\\\\Users\\\\ganad\\\\Desktop\\\\Learning with Projects\\\\GenerativeAI_FreeCodeCamp-1\\\\𝐃𝐀𝐘_𝟒_𝐇𝐮𝐠𝐠𝐢𝐧𝐠𝐅𝐚𝐜𝐞_𝐀𝐏𝐈_&_𝐋𝐚𝐧𝐠𝐜𝐡𝐚𝐢𝐧\\\\EEP567_FFMMMTI_Final_Report (1).pdf', 'page': 7}),\n",
       " Document(page_content=\"9  \\nFigure 4: YOLOv8s (left) vs YOLOv8x (right) Prediction Validation Batch  \\nProof of concept  \\nTo showcase the capabilities of our system, two proof of concept products were developed. The first \\nproduct is a user -friendly web application that leverages Streamlit and allows for real -time video \\nprocessing, as well as image and recorded video analysis.  This interface allows users to interact with the \\nsystem upload live video feeds and quickly receive detection or segmentation results to differentiate \\nbetween friends and foes. The application utilizes the YOLOv8 model for timely object detection and \\nsegm entation focusing on speed and accuracy.  Although the current iteration of the web app lacks a \\nchallenge -response feature to authenticate identified targets due to Streamlit's limitations with external \\nservers, this approach significantly enhances accessibility for end -users, particularly militar y personnel. \\nThis collaborative effort demonstrates how the fusion of advanced machine learning models with user -\\ncentric applications can enhance critical decision -making processes.  The second and last proof of conc ept \\nproduct aims to showcase the real -time capabilities of our system by integrating the CRS code with a \\nPython script designed to perform inference on webcam video streams. This seamless integration with the \\nwebcam simulates realistic deployment scenarios, while leveraging socket connections for Challenge \\nResponse functionality, emulating software -defined radio setups. The script operates in AI mode, wherein \\nthe AI model processes each frame to detect and classify camouflage uniforms as friendly or hostile . The \\nchallenge -response system allows manual operation, with users issuing commands to challenge targets, \\nprompting identity confirmation within a set timeframe. This demonstration not only highlights  the \\naccuracy and robustness of the model but also hints at potential integration with external \\nsoftware/hardware components.  \\nFuture Work  \\nOur project will tackle key features and challenges for future growth and deployment. First, in order to \\ncapture a broader array of soldiers in the field and under varying lighting conditions, the team intends to\", metadata={'source': 'C:\\\\Users\\\\ganad\\\\Desktop\\\\Learning with Projects\\\\GenerativeAI_FreeCodeCamp-1\\\\𝐃𝐀𝐘_𝟒_𝐇𝐮𝐠𝐠𝐢𝐧𝐠𝐅𝐚𝐜𝐞_𝐀𝐏𝐈_&_𝐋𝐚𝐧𝐠𝐜𝐡𝐚𝐢𝐧\\\\EEP567_FFMMMTI_Final_Report (1).pdf', 'page': 8}),\n",
       " Document(page_content='10 significantly enlarge the dataset. Additionally, we plan to take a meticulous approach when selecting the \\naugmentation techniques for our dataset to not only enlarge the dataset further but also to allow the model \\nto generalize better by addressing the iss ues noted in the experimental results and analysis section. \\nFollowing this development, the computer vision model will be able to function more consistently in a \\nrange of real -world circumstances thanks to its improved resilience. We also aim to optimize t he \\ncomputer vision model by incorporating ensemble learning to draw additional insight from the data and \\nincrease classification accuracy and overall robustness. Looking beyond our current scope for military \\noperations, we believe that our concept extends to law enforcement, disaster response, and perhaps \\nsearch -and-rescue missions. Our project aspires to offer improved situational awareness by providing \\nrapid identification and classification which in turn leads to safer and more effective operations in th e \\nfuture.    \\nReferences  \\n[1] “Yolov8: How to Train for Object Detection on a Custom Dataset.” YouTube , Roboflow, 10 Jan. \\n2023, www.youtube.com/watch?v=wuZtUMEiKWY&t=962s.  \\n[2] Burhan -Q, Laughing -q and glenn -jocher (2024) Model Training with Ultralytics YOLO , Train - \\nUltralytics YOLOv8 Docs . Available at: https://docs.ultralytics.com/modes/train/#train -settings \\n(Accessed: 11 February 2024).  \\n[3] Johnson, Reed. “Roboflow Inference: Effortless Local Computer Vision Deployment with Python.” \\nRoboflow Blog , Roboflow Blog, 8 Sept. 2023, blog.roboflow.com/inference -python/#roboflow -inference -\\nlocal -deployment -made -easy.  \\n[4] Shrader, C. R. (n.d.). The US Army War College Quarterly: Parameters. The US Army War College \\nQuarterly: Parameters The US Army War College Quarterly: Parameters. \\nhttps://press.armywarcollege.edu/cgi/viewcontent.cgi?article=1645&context=parameters  \\n[5] SkalskiP. “YOLOv8 LIVE.” GitHub , github.com/SkalskiP/yolov8 -live/tree/master. Accessed 15 Feb. \\n2024.  \\n[6] Ultralytics. “Ultralytics YOLOv8.” GitHub , github.com/ultralytics/ultralytics. Accessed 1 7 Feb. 202 4.  \\n[7] Verganti, R., Vendraminelli, L. and Iansiti, M. (2020), Innovation and Design in the Age of Artificial \\nIntelligence. J Prod Innov Manag, 37: 212 -227. https://doi.org/10.1111/jpim.12523  \\n \\nNOTE: CRS Sources are listed in Jupyter Notebook, image sources are listed in Github, with the sources \\nbelow in reference only to this document.  \\nAppendix  \\n \\nProject Repository:  \\nhttps://github.com/Naif -Ganadily/Friend -or-Foe-Multi -Modal -Military -Target -Identification', metadata={'source': 'C:\\\\Users\\\\ganad\\\\Desktop\\\\Learning with Projects\\\\GenerativeAI_FreeCodeCamp-1\\\\𝐃𝐀𝐘_𝟒_𝐇𝐮𝐠𝐠𝐢𝐧𝐠𝐅𝐚𝐜𝐞_𝐀𝐏𝐈_&_𝐋𝐚𝐧𝐠𝐜𝐡𝐚𝐢𝐧\\\\EEP567_FFMMMTI_Final_Report (1).pdf', 'page': 9})]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testingopenai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
